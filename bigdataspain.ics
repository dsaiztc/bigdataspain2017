BEGIN:VCALENDAR
PRODID:ics.py - http://git.io/lLljaA
VERSION:2.0
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T095500Z
DTEND:20171116T100000Z
SUMMARY:Welcome [Business]
DESCRIPTION:Title:\nWelcome\n\nSummary:\nWelcome Note\n\nDescription:\n\n\nSpeaker:\nName: \nPosition:  @ \nDescription: \nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 25
UID:74185561-3712-4aae-be61-528ba05bc43e@7418.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T100500Z
DTEND:20171116T103500Z
SUMMARY:Artificial Intelligence and Data-centric businesses [Business]
DESCRIPTION:Title:\nArtificial Intelligence and Data-centric businesses\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Óscar Mendez\nPosition: CEO and Co-founder @ Stratio\nDescription: Oscar Mendez Soto is CEO and co-founder of Stratio\, a pioneering Big Data product company founded in 2014. He has more than 15 years of experience launching and managing successful companies and has accrued extensive knowledge in the field of Big Data and Artificial Intelligence over the last two decades. Oscar has always envisaged where technology is going next and has been an early adopter of Big Data technologies that provide massive data processing capabilities. He is the founder of Big Data Spain\, the second biggest Big Data conference in Europe.\nLinkedIn: https://www.linkedin.com/in/oscar-m%C3%A9ndez-soto-8a00a7b/\nTwitter: https://twitter.com/omendezsoto\n        \n        
LOCATION:Theatre 25
UID:7ee46757-4629-4ea0-bc66-b8b66178d821@7ee4.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T104500Z
DTEND:20171116T112000Z
SUMMARY:Big Data\, Analytics\, and Tax Fraud [Business]
DESCRIPTION:Title:\nBig Data\, Analytics\, and Tax Fraud\n\nSummary:\n\n\nDescription:\nNone\n\nSpeaker:\nName: D. José Borja Tomé\nPosition: Deputy Director @ Agencia Tributaria\nDescription: \nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 25
UID:fe936053-2024-4381-9408-34a997c0cd01@fe93.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T112500Z
DTEND:20171116T120000Z
SUMMARY:Roundtable – Thursday [Business]
DESCRIPTION:Title:\nRoundtable – Thursday\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Óscar Mendez\nPosition: CEO and Co-founder @ Stratio\nDescription: Oscar Mendez Soto is CEO and co-founder of Stratio\, a pioneering Big Data product company founded in 2014. He has more than 15 years of experience launching and managing successful companies and has accrued extensive knowledge in the field of Big Data and Artificial Intelligence over the last two decades. Oscar has always envisaged where technology is going next and has been an early adopter of Big Data technologies that provide massive data processing capabilities. He is the founder of Big Data Spain\, the second biggest Big Data conference in Europe.\nLinkedIn: https://www.linkedin.com/in/oscar-m%C3%A9ndez-soto-8a00a7b/\nTwitter: https://twitter.com/omendezsoto\n        \n        
LOCATION:Theatre 25
UID:8fea13df-b992-4421-8532-de97103b7d66@8fea.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T120000Z
DTEND:20171116T123000Z
SUMMARY:More people\, less banking: Blockchain [Business]
DESCRIPTION:Title:\nMore people\, less banking: Blockchain\n\nSummary:\n- Blockchain\n\nDescription:\nPrimary function of banking sector is promoting economic activity\; which means “commerce”\, exchanging what someone produces-has for something that someone consumes-desires. That’s something that humans have always being doing. At the very beginning through real asset trading ( bartering ) and afterwards through a fiduciary\, trustable system ( FIAT ). Banking is all about commerce and that’s why a big change in commerce is always followed by a big change in banking. We are at the verge of a huge change in commerce ( blockchain ) that will be followed by a huge change in finance: 2getherbank.\n\nSpeaker:\nName: Salvador Casquero Algarra\nPosition: Co-Founder @ 2getherbank\nDescription: Salvador has over 25 years of experience in banking. Salvador has developed his career in JP Morgan London\, BBVA\, La Caixa and Banco Sabadell as FX Trading and Electronic Business Director. He has a Industrial engineering degree in La Universidad Pontificia de Comillas (ICAI). Casquero has been researching and building concepts over the Blockchain since 2010. Areas of responsibility in 2getherbank includes: technology platform\, Blockchain developments\, API Marketplace\, Compliance and Innovation.\nLinkedIn: https://www.linkedin.com/in/salvador-casquero-algarra-9607071b/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:2f2548a0-45bf-48af-97ab-61443147ed74@2f25.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T121500Z
DTEND:20171116T124500Z
SUMMARY:Human-in-the-loop: a design pattern for managing teams which leverage ML [Technical]
DESCRIPTION:Title:\nHuman-in-the-loop: a design pattern for managing teams which leverage ML\n\nSummary:\nHuman-in-the-loop is an approach which has been used for simulation\, training\, UX mockups\, etc.\n\nDescription:\n\n\nSpeaker:\nName: Paco Nathan\nPosition: Director\, Learning Group @ O'Reilly\nDescription: None\nLinkedIn: https://www.linkedin.com/in/ceteri/\nTwitter: https://twitter.com/pacoid\n        \n        
LOCATION:Theatre 25
UID:a7072b0a-7d5d-4d29-9a49-a1e97c57ff41@a707.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T121500Z
DTEND:20171116T124500Z
SUMMARY:Big Data security: Facing the challenge [Technical]
DESCRIPTION:Title:\nBig Data security: Facing the challenge\n\nSummary:\n- Secure data processing\n\nDescription:\nA data-centric platform integrates multiple Big Data open source technologies. For example\, at Stratio we use Spark\, Kafka\, Elastic search and many more. Most of these technologies do not offer native security. This lack of security\, not only leaves companies open to critical risks like data leakage\, unsecure communications or DoS attacks but is also a major barrier to complying with different regulations such as LOPD\, PCI-DSS or the upcoming GDPR. This talk gives a technical and innovative overview of how companies can face the challenge of protecting the data and services that are in their data-centric platform\, focusing on three main aspects: implementing network segmentation\, managing AAA and securing data processing.\n\nSpeaker:\nName: Carlos Gomez\nPosition: Big Data Architect @ Stratio\nDescription: Carlos Gómez Cainzos is a Big Data architect in the Architecture and Security team at Stratio. He is passionate about systems’ security. He focuses on obtaining secure\, stable and scalable open-source solutions for large scale distributed systems. He is an early adopter of Big Data security technologies\, up to date on the latest trends to adapt to current business needs.\nLinkedIn: https://www.linkedin.com/in/carlosgomezcainzos/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:7313eccc-31e0-43bc-93b4-51754c5fbd4a@7313.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T121500Z
DTEND:20171116T124500Z
SUMMARY:How ICIJ cracked the Panama and the Paradise Papers [Business]
DESCRIPTION:Title:\nHow ICIJ cracked the Panama and the Paradise Papers\n\nSummary:\nLeaked data is getting to journalists at a massive scale and they're too\, using technology and big data techniques to expose wrongdoing and corruption\, like in the Panama and Paradise Papers investigations. \n\nDescription:\n\nInvolving 2.6 TB of data and 11.5 million documents\, the Panama Papers was the largest amount of leak files and cross-border investigation in journalism history. For one year\, more than 400 reporters across 80 countries dived into this massive trove of information that exposed how the offshore economy works.\n\n\nSpeaker:\nName: Mar Cabra\nPosition: Data editor @ International Consortium of Investigative Journalists\nDescription: \nLinkedIn: https://www.linkedin.com/in/mar-cabra/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:c8bff513-17cc-4a61-8c3a-aeb081b4ad98@c8bf.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T121500Z
DTEND:20171116T124500Z
SUMMARY:Scaling to billions of Edges in a Graph Database [Technical]
DESCRIPTION:Title:\nScaling to billions of Edges in a Graph Database\n\nSummary:\n- Scalability\n\nDescription:\nNone\n\nSpeaker:\nName: Michael Hackstein\nPosition: Senior Graph Specialist @ ArangoDB GmbH\nDescription: Michael Hackstein holds a Masters degree in Computer Science and is the creator of ArangoDBs graph capabilities. During his academic career he focused on complex algorithms and especially graph databases. Michael is an internationally experienced speaker who loves salad\, cake and clean code.\nLinkedIn: \nTwitter: https://twitter.com/mchacki\n        \n        
LOCATION:Theatre 20
UID:ac560a58-c7b4-4aac-a0f2-b05500b0fe3c@ac56.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T125500Z
DTEND:20171116T132500Z
SUMMARY:Apache Spark Machine Learning [Technical]
DESCRIPTION:Title:\nApache Spark Machine Learning\n\nSummary:\nThis talk will look at the architecture of PySpark as it is today\, how its changing (and why)\, and what this means in the "Big Data" space.\n\nDescription:\nMany of the recent big data systems\, like Hadoop\, Spark\, Kafka\, and more\, are written primarily in JVM languages. At the same time their is a wealth of tools for data science and data analytics that exist outside of the JVM. This talk will look at how we can bridgethe gap today using PySpark\, as well as how other systems (like Kafka Streams) can or are bridging the gap and explore the challenges of pure Python solution like dask.\n\nSpeaker:\nName: Holden Karau\nPosition: Committer Apache Spark @ Google\nDescription: Holden is an Apache Spark committer\, and an active open source contributor. Holden talks internationally on Spark and holds office hours at coffee shops at home and abroad. She makes frequent contributions to Spark\, specializing in PySpark and Machine Learning. Prior to IBM she worked on a variety of distributed\, search\, and classification problems at Alpine\, Databricks\, Google\, Foursquare\, and Amazon. She graduated from the University of Waterloo with a Bachelor of Mathematics in Computer Science. Outside of software she enjoys playing with fire\, welding\, scooters\, poutine\, and dancing.\nLinkedIn: https://www.linkedin.com/in/holdenkarau/\nTwitter: https://twitter.com/holdenkarau\n        \n        
LOCATION:Theatre 25
UID:021c935f-a0a9-40f6-8fb0-dbf2f290af1a@021c.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T125500Z
DTEND:20171116T132500Z
SUMMARY:Elasticsearch (R)Evolution — You Know\, for Search… [Technical]
DESCRIPTION:Title:\nElasticsearch (R)Evolution — You Know\, for Search…\n\nSummary:\nElasticsearch is a distributed\, RESTful search and analytics engine built on top of Apache Lucene. After the initial release in 2010 it has become the most widely used full-text search engine\, but it is not stopping there.  The revolution happened and now it is time for evolution. We dive into current improvements and new features — how to make a great product even better.\n\nDescription:\nElasticsearch is a distributed\, RESTful search and analytics engine built on top of Apache Lucene. After the initial release in 2010 it has become the most widely used full-text search engine\, but it is not stopping there. The revolution happened and now it is time for evolution. We dive into current improvements and new features — how to make a great product even better.\n\nSpeaker:\nName: Philipp Krenn\nPosition: Infrastructure | Developer Advocate @ Elastic\nDescription: Philipp is part of the infrastructure team and a developer advocate at Elastic. He is frequently talking about full-text search\, databases\, operations\, and security. Additionally\, he is organizing multiple meetups in Vienna.\nLinkedIn: https://www.linkedin.com/in/philippkrenn/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:2da816d8-ebeb-4e43-b620-c660db7dc287@2da8.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T125500Z
DTEND:20171116T132500Z
SUMMARY:Disaster Recovery for Big Data [Technical]
DESCRIPTION:Title:\nDisaster Recovery for Big Data\n\nSummary:\nAll modern Big Data solutions\, like Hadoop\, Kafka or the rest of the ecosystem tools\, are designed as distributed processes and as such include some sort of redundancy for High Availability. For mission-critical data this is often not good enough and enterprises seek a way to avoid having all their eggs in one basket.  In this talk we will give examples of possible Disaster Recovery options\, how to integrate them with existing solutions and what are the benefits of having them.\n\nDescription:\nBig Data is all about unifying data in one place. Enterprises spend a lot of time setting up data flows from their productive environments to these new platforms to ensure that every piece of relevant information is stored in a single location. This has the benefit of providing a convenient centralized location for data access\, a much broader view of what is happening\, and allows for more efficient data cleansing and ultimately better analytics. However\, there is a downside to this: the Big Data platform becomes mission critical and\, if kept in a single location\, might become a very vulnerable spot in the company’s structure. We all usually associate Disaster Recovery with the “What if your datacenter gets blown up by a bomb?”\, but a broken water pipe or a power cable being cut down by accident are much more common and might still take a lot of time to recover. Big Data solutions need to be ready to be launched in a different datacenter if needed\, just like the rest of the company’s solutions. So\, how’s the Big Data Platform different from the rest of the IT services that it cannot be simply incorporated into an existing Disaster Recovery solution? First\, productive Big Data solutions are generally run in their own environment: dedicated hardware\, dedicated network\, and a specific platform management solution such as Cloudera Manager or Apache Ambari. Secondly\, the sheer volume of data and the complexity of the internal processes in these clusters add to the difficulty of designing a DR solution for Big Data and generally just the simplest scripts (with lots of manual intervention) are implemented.\n\nSpeaker:\nName: Carlos Izquierdo\nPosition: IT & Big Data Architect @ Datatons\nDescription: Carlos Izquierdo is a Big Data architect focused on providing robust\, scalable solutions at a reasonable cost. He likes tinkering with technology and discovering new tools\, so he is quite up to date in IT needs and trends. He has worked in businesses big and small\, as well as in academic environments. He always prefers open source and free software solutions\, which he has been using for several years. His experience managing Big Data platforms allows him to provide insightful and hands-on solutions related to Hadoop and its ecosystem. Three years ago he founded a Data-oriented startup called Datatons.\nLinkedIn: https://www.linkedin.com/in/gheesh/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:6c30e457-6729-4df9-8a2a-47b278d71c90@6c30.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T134000Z
DTEND:20171116T141000Z
SUMMARY:Keeping your Enterprise’s Big Data Secure [Technical]
DESCRIPTION:Title:\nKeeping your Enterprise’s Big Data Secure\n\nSummary:\nSecurity is a tradeoff between usability and safety and should be driven by the perceived threats.\n\nDescription:\nEnterprises are putting all of their business-critical information in Hadoop. When your data analytics team is small\, security is an afterthought\, but as your data and team grow\, security becomes imperative. Implementing security always requires tradeoffs between protection and convenience and thus it is important to understand the threats and how to protect against them.\n\nSpeaker:\nName: Owen O’Malley\nPosition: Co-Founder and Technical Fellow @ Hortonworks\nDescription: Co-founder of Hortonworks and senior architect.\nLinkedIn: https://www.linkedin.com/in/owenomalley/\nTwitter: https://twitter.com/owen_omalley\n        \n        
LOCATION:Theatre 25
UID:95ee6a90-75c9-4414-b962-f74e141d8d5d@95ee.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T134000Z
DTEND:20171116T141000Z
SUMMARY:Deep reinforcement learning : Starcraft learning environment [Technical]
DESCRIPTION:Title:\nDeep reinforcement learning : Starcraft learning environment\n\nSummary:\nA theorical description of reinforcement learning principles and a deep dive into DeepMind Research environment .\n\nDescription:\nDeepMind released Pysc2\, python package for reinforcement learning in Starcraft 2 environment in summer 2017. The talk includes an exhaustive description of reinforcement learning concepts for scripted agents and dives into a owned open source mini-game map contribution by talk´s author. The talk includes a Demo\n\nSpeaker:\nName: Gema Parreño Piqueras\nPosition: Data Scientist @ BBVA\nDescription: \nLinkedIn: https://www.linkedin.com/in/gemaparreno/detail/photo/\nTwitter: https://twitter.com/SoyGema\n        \n        
LOCATION:Theatre 19
UID:ab848f56-e6f5-4fef-adaf-a9dc4ed298c5@ab84.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T134000Z
DTEND:20171116T141000Z
SUMMARY:Apache Spark vs rest of the world – Problems and Solutions [Technical]
DESCRIPTION:Title:\nApache Spark vs rest of the world – Problems and Solutions\n\nSummary:\nApache Spark is a great solution for building Big Data applications. It provides really fast SQL-like processing\, machine learning library\, and streaming module for near real time processing of data streams. Unfortunately\, during application development and production deployments we often encounter many difficulties in mixing various data sources or bulk loading of computed data to SQL or NoSQL databases. All in all\, there are a lot of challenges at the confluence of Apache Spark and the rest of the Big Data world\, including HBase\, Hive\, PostgreSQL or Kafka. Those are the issues that we will discuss in our presentation.\n\nDescription:\nApache Spark is a great solution for building Big Data applications. It provides really fast SQL-like processing\, machine learning library\, and streaming module for near real time processing of data streams. Unfortunately\, during application development and production deployments we often encounter many difficulties in mixing various data sources or bulk loading of computed data to SQL or NoSQL databases. All in all\, there are a lot of challenges at the confluence of Apache Spark and the rest of the Big Data world\, including HBase\, Hive\, PostgreSQL or Kafka. Those are the issues that we will discuss in our presentation.\n\nSpeaker:\nName: Arkadiusz Jachnik\nPosition: Senior Data Mining Specialist / Senior Data Scientist @ Agora SA\nDescription: Mr. Arkadiusz Jachnik is a Senior Data Scientist at Big Data Department of Agora S.A. – one of the biggest media company in Poland. He is currently working on real-time user profiling system and highly scalable recommendation platform. He received his BSc and MSc in Computer Science at the Poznan University of Technology. Mr. Jachnik is an author and coauthor of several machine learning publications. His current research activity concerns multi-label classification and multi-output prediction.\nLinkedIn: https://www.linkedin.com/in/arkadiusz-jachnik-7459b354/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:5b400134-3d34-4242-b9c9-df865d7a1d8e@5b40.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T134000Z
DTEND:20171116T141000Z
SUMMARY:Relational is the new Big Data [Technical]
DESCRIPTION:Title:\nRelational is the new Big Data\n\nSummary:\nRelational databases were the persistence system of choice for decades\, until the Web 2.0 in the 2000s required to process volumes of data so big it needed distributed systems running in parallel. A new type of databases (NoSQL) was adopted to solve this problem in different ways.  We are now seeing the pendulum swing back\, with some relational databases evolving into systems that can easily be made distributed\, keeping their versatility\, simplicity in structure and easy infrastructure maintenance. We will showcase how Citus can be combined with PostgreSQL to work with distributed data.\n\nDescription:\nIn this talk we will start by explaining how relational databases (RDBMSs) were created in the 70s\, replacing the previous hierarchical IMS systems\, and developed in the next 40 years\, being the persistence systems of reference during that time. In the 2000s\, the arrival of the Web 2.0 brought an explosion of the volume of data to persist\, analyze and process – what was to be called Big Data. The number of users was now in the hundreds of millions or billions\, amounts of data stored went up to Tera and Petabytes and the data persisted had to be queried and analysed within seconds\, enabling real-time experiences for the user. Scaling relational databases horizontally and keeping transactions ACID was proven to be a very hard process. We will talk about some of the problems to face when sharding relational data into different machines\, like deadlocks\, two-phase commits or parallelization. To solve these problems\, a few new paradigms emerged and were developed\, which resulted in new ways to persist and process big volumes of data\, what was called NoSQL databases. We will describe some of these paradigms\, like Big Table from Google or LinkedIn Voldemort key-value store. We will also cover the main types of NoSQL databases following the CAP theorem and main differences between them. At this point companies rushed into exploring these new systems\, using these technologies to build new products that were highly performant and scalable\, solving their Big Data needs. Some of these companies had millions or billions of users\, and dedicated whole teams to architect and maintain NoSQL databases and other distributed technologies like Hadoop and Spark. But these systems can be very complex to set up and maintain. The amount of human and technical resources required to properly having them running is not trivial\, and this can be problematic in a lot of companies that can not dedicate this level of resources just to have their data pipeline up and running. Another drawback of NoSQL databases is that they are very specific in the way they function and the problem they solve. So even if cloud providers now offer implementations of NoSQL databases that they run\, monitor and scale\, the team of developers writing the applications using those databases have to understand exactly how they will be configured and set up\, and a change in the model or the way the application works (even changing the structure of the queries) might require starting from scratch.\n\nSpeaker:\nName: Miguel Ángel Fajardo\nPosition: VP of Technology @ Geoblink\nDescription: Miguel Ángel Fajardo is the CTO of Geoblink\, a Spanish startup that is changing how businesses harness the power of data through Location Analysis. He has extensive experience leading teams and pushing for innovation in both sides of the Atlantic in different industries like telecom\, media\, e-commerce and videogames.\nLinkedIn: https://www.linkedin.com/in/mifajardo/\nTwitter: https://twitter.com/ma_bits\n        \n        
LOCATION:Theatre 20
UID:9e04a927-9412-4a20-8165-b55e9ea3126f@9e04.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T143000Z
DTEND:20171116T150000Z
SUMMARY:Why big data didn’t end causal inference [Technical]
DESCRIPTION:Title:\nWhy big data didn’t end causal inference\n\nSummary:\n\n\nDescription:\nTen years ago there were rumours of the death of causal inference. Big data was supposed to enable us to rely on purely correlational data to predict and control the world. In this talk\, I argue that the rumours were strongly exaggerated. Causal inference is becoming increasingly relevant thanks to improvements in inference methods and–ironically–the availability of data. Far from becoming marginalised\, causal inference is today more relevant than it’s ever been.\n\nSpeaker:\nName: Totte Harinen\nPosition: Data Scientist II @ Uber\nDescription: Totte works at the intersection of data science and behavioral research. He’s passionate about all things causality: randomised experiments\, quasi-experiments\, and causal modeling. At Uber\, his job is to help EATS and ridesharing teams across the world to apply these methods to understand what works and what doesn’t. Totte has a PhD on a causal explanation from King’s College London.\nLinkedIn: https://www.linkedin.com/in/tharinen/?ppe=1\nTwitter: https://twitter.com/totteh\n        \n        
LOCATION:Theatre 25
UID:cb617c64-10b6-40a7-a890-a76ffa4403ab@cb61.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T143000Z
DTEND:20171116T150000Z
SUMMARY:Building Google’s ML Engine from Scratch on AWS with GPUs\, Kubernetes\, Istio\, and TensorFlow [Technical]
DESCRIPTION:Title:\nBuilding Google’s ML Engine from Scratch on AWS with GPUs\, Kubernetes\, Istio\, and TensorFlow\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Chris Fregly\nPosition: Founder and Research Engineer @ PipelineAI\nDescription: Chris Fregly is Founder and Research Engineer at PipelineAI\, a Streaming Machine Learning and Artificial Intelligence Startup based in San Francisco.  He is also an Apache Spark Contributor\, a Netflix Open Source Committer\, founder of the Global Advanced Spark and TensorFlow Meetup\, author of the O’Reilly Training and Video Series titled\, “High Performance TensorFlow in Production.”\nLinkedIn: https://www.linkedin.com/in/cfregly/\nTwitter: https://twitter.com/cfregly\n        \n        
LOCATION:Theatre 19
UID:d53d6996-c247-4071-add2-6c1a30da54ff@d53d.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T143000Z
DTEND:20171116T150000Z
SUMMARY:Fishing Graphs in a Hadoop Data Lake [Technical]
DESCRIPTION:Title:\nFishing Graphs in a Hadoop Data Lake\n\nSummary:\n- Smartgraphs\n\nDescription:\nHadoop clusters can store nearly everything in a cheap and blazingly fast way to your data lake. Answering questions and gaining insights out of this ever growing stream becomes the decisive part for many businesses. Increasingly data has a natural structure as a graph\, with vertices linked by edges\, and many questions arising about the data involve graph traversals or other complex queries\, for which one does not have an a priori given bound on the length of paths.\n\nSpeaker:\nName: Jörg Schad\nPosition: Distributed Systems Engineer @ Mesosphere\nDescription: Joerg Schad is a Distributed Systems Engineer at Mesosphere who works on DC/OS and Apache Mesos. Prior to this he worked on SAP Hana and in the Information Systems Group at Saarland University. His passions are distributed (database) systems\, data analytics\, and distributed algorithms and his speaking experience include various Meetups\, international conferences\, and lecture halls.\nLinkedIn: https://www.linkedin.com/in/j%C3%B6rg-schad-945345a9/?ppe=1\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:4a5d1ac8-df87-4f02-a8c7-b1ae98edd82b@4a5d.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T143000Z
DTEND:20171116T150000Z
SUMMARY:End of the Myth: Ultra-Scalable Transactional Management [Technical]
DESCRIPTION:Title:\nEnd of the Myth: Ultra-Scalable Transactional Management\n\nSummary:\n- SQL\n\nDescription:\nThe talk will focus on explaining why operational databases do not scale due to limitations in legacy transactional management. It will provide an overview of how an operational database works and the underlying transactional management sub-system. It will clarify the misunderstandings created around the scalability of transactional management and the CAP theorem. In the second part\, it will cover how the lack of scalability of operational databases has been solved by LeanXcale. Finally\, it will address a number of use cases from different verticals that can benefit from a scalable database and how their architecture can be simplified.\n\nSpeaker:\nName: Dr. Ricardo Jimenez-Peris\nPosition: CEO and Founder @ LeanXcale\nDescription: Dr. Ricardo Jimenez-Peris\, CEO and Founder of LeanXcale\, was a professor and researcher in distributed systems and distributed data management for over 20 years and recently abandoned his research and academic career to bring LeanXcale technology to the market. Dr. Jimenez has been an invited speaker at top tech companies in Silicon Valley such as Facebook\, Twitter\, Salesforce\, Cloudera\, HortonWorks\, MapR\, HP\, Heroku\, Microsoft\, EMC-Greenplum (now EMC-Pivotal)and others in recent years. He is also a member of the cloud experts committee advising the European Commission on the next research challenges in cloud\, software\, and services. He has co-authored a book on database replication\, and over 150 research papers and has been co-inventor of two revolutionary database patents.\nLinkedIn: https://www.linkedin.com/in/ricardojimenezperis/\nTwitter: https://twitter.com/RicardoJimenezP\n        \n        
LOCATION:Theatre 20
UID:e49b96e2-c964-4be7-80d5-a78a985562a5@e49b.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T151000Z
DTEND:20171116T154000Z
SUMMARY:AI: The next frontier [Technical]
DESCRIPTION:Title:\nAI: The next frontier\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Amparo Alonso\nPosition: Professor @ University of A Coruña\nDescription: Amparo Alonso-Betanzos is a Professor at the Department of Computer Science and coordinator of the Laboratory for Research and Development in Artificial Intelligence at the University of A Coruña. She is currently working on the development of Machine Learning algorithms\, and on their applications to several fields\, such as predictive maintenance on engineering or prediction of gene expression in bioinformatics.\nLinkedIn: https://www.linkedin.com/in/amparo-alonso-80516211/\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:e2c96d54-9c38-48ec-9610-b6c6c72a6208@e2c9.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T151000Z
DTEND:20171116T154000Z
SUMMARY:Towards biologically plausible deep learning [Technical]
DESCRIPTION:Title:\nTowards biologically plausible deep learning\n\nSummary:\n- Deep learning\n\nDescription:\nThe recent success of deep structured learning in areas as image and speech recognition\, and problem solving in general\, has made many people wonder if neural networks based artificial intelligence has surpassed the capabilities of humans intelligence in these specific applications. Others\, however\, argue that the more advanced network topologies that have emerged in the recent years are merely more sophisticated statistical techniques for fitting functions. In this session we’ll make a review of where deep learning currently stands\, what are the key present limitations and challenges\, and how neuroscience and psychology can bring us closer to human-level intelligence.\n\nSpeaker:\nName: Nikolay Manchev\nPosition: Data Scientist @ IBM\nDescription: Nikolay Manchev is a Data Scientist in the EMEA CDS team at IBM and a research student at King’s College London. He specializes in Machine Learning and Data Science. He is a speaker\, blogger\, author of numerous articles\, and member of advisory board of the Spark Technology Center. For the last four years Nikolay has been working exclusively in the big data space with focus on custom machine learning algorithms and large scale data processing. He has an M.Sc. in Software Technologies\, M.Sc. in Data Science (City\, University of London)\, and runs the London Machine Learning Study Group meetup.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 19
UID:f4251ccc-dc8d-4f7b-9bae-24b7c147d7cf@f425.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T151000Z
DTEND:20171116T154000Z
SUMMARY:Towards an Unified API for Spark and the IIoT [Technical]
DESCRIPTION:Title:\nTowards an Unified API for Spark and the IIoT\n\nSummary:\nStructured Streaming is a game changer for Apache Spark having a unified API for both batch and real-time processing. Moreover\, its support for “event time” and watermarking simplifies its deployment on IIoT related projects. In this workshop\, we will hands-on Spark´s Structured Streaming API and more specifically on its advantages for the IIoT domain.\n\nDescription:\nThe Industrial Internet of Things (IIoT) nowadays is an exploding trend with significant implications for the global economy. It spans industries including manufacturing\, mining\, agriculture\, oil and gas\, and utilities. It also encompasses companies that depend on durable physical goods to conduct business\, such as organizations that operate hospitals\, warehouses and ports or that offer transportation\, logistics and healthcare services. Not surprisingly\, the IIoT’s potential payoff is enormous. A specific example of this potential use is the Predictive maintenance of assets\, saving over scheduled repairs\, reducing overall maintenance costs and eliminating breakdowns up. For example\, Thames Water\, one of the largest providers of water in the UK\, is using sensors and real-time data to help the utility company anticipate equipment failures and respond more quickly to critical situations\, such as leaks or adverse weather events. However\, analyzing such large quantities of usually out-of-order real-time data from different sensors and system is a real challenge with current real-time Big Data analytics frameworks. In order to help on addressing use cases such as the previously described where real-time processing of IIoT data is needed\, efforts of the Spark community has led to the development of the Structured Streaming API that provides a unified view of both real-time and batch processing paradigms. This API provides latest performance gains from the Catalyst optimizer presented on Spark 2.0 and its support for event-time and late out of order data using watermarking techniques enables easier development of IIoT projects where data usually reaches the cloud late and in unordered fashion. Moreover\, the latest release (Spark 2.2) provides support for maintaining custom state between the different micro-batches of the real-time data allowing more complex use cases. On this hands-on workshop\, we will use Apache Zeppelin\, Apache Kafka\, Apache Avro\, Apache Cassandra and Spark´s Structured Streaming API to solve challenges of related to IIoT projects such as handling late unordered data. More specifically\, we will go through real code ingesting simulated late out- of-order data from Kafka\, processing it with Spark´s Structured Streaming API and saving the results of those computations on Cassandra while we explore the Structured Streaming API\, its main features such the unified view of real-time and batch analytics or its support of out-of-order late data processing using watermarking\, benefits for Spark developers and its current limits for these kind of projects. All the code of the workshop and the developer environment be available at GitHub right after the workshop using Docker.\n\nSpeaker:\nName: Angel Conde\nPosition: DevOps and Data Engineer @ IK4-Ikerlan\nDescription: Angel Conde is a DevOps and Data Engineer at IK4-Ikerlan where he works building Big Data platforms for the Industry 4.0 based on Apache Spark technology.\nLinkedIn: https://www.linkedin.com/in/acmanjon/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:e2d42c59-c266-40e7-b555-96d40eb31e23@e2d4.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T151000Z
DTEND:20171116T154000Z
SUMMARY:Tuning Java Driver for Apache Cassandra [Technical]
DESCRIPTION:Title:\nTuning Java Driver for Apache Cassandra\n\nSummary:\n- Cassandra\n\nDescription:\nApache Cassandra is distributed masterless column store database which is becoming mainstream for analytics and IoT data. Many use cases where Cassandra is natural fit require latency tuning in order to serve requests really fast. DataStax driver has many options\, some less familiar\, which can greatly influence performance aspect. This talk will focus on Java applications and options at your disposal in DataStax Java driver which became standard when you want to use this database. We will concentrate on both monitoring and tuning aspect of things and we will provide different options for different use cases. There is no silver bullet solution and having many options requires deep dive when you want to figure out right decision. This talk will narrow down options and give you push in the right direction.\n\nSpeaker:\nName: Nenad Bozic\nPosition: Co Founder @ SmartCat\nDescription: Big Data enthusiast and Apache Cassandra fan. DataStax MVP for Apache Cassandra for 2017. Craftsman with more than 10 years of experience\, all arounder but when he does backend coding (mostly in Java) he feels right at home. Strong believer in balance between good technical skills and soft skills. Striving for knowledge is his main drive\, which is why he enjoys learning new tools and languages\, blogging\, working on open source\, presenting at conferences. Co-founder of SmartCat company delivering Big Data consulting\, Data Science services and DevOps. Family guy\, most of his free time tries to spend with his wife and daughter but also likes boardgaming\, snowboarding and playing tennis and football.\nLinkedIn: https://www.linkedin.com/in/nenadbozic/\nTwitter: https://twitter.com/NenadBozicNs\n        \n        
LOCATION:Theatre 20
UID:6502e6ae-12c5-42db-88c0-2c980e8ba34a@6502.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T155500Z
DTEND:20171116T162500Z
SUMMARY:Big Data\, Big Quality? [Technical]
DESCRIPTION:Title:\nBig Data\, Big Quality?\n\nSummary:\nThe journey of Big Data Quality at Spotify\n\nDescription:\nIrene will drive you through the process held at Spotify in terms of data quality: from why and how we became aware of the importance of quality\, to which main products we have developed to tackle our primary quality problems. The presentation will conclude with details on where Spotify is now and what is our future strategy.\n\nSpeaker:\nName: Irene Gonzálvez\nPosition: Technical Product Manager @ Spotify\nDescription: Irene is passionate about innovation and the transformation of business values and customers’ needs into new technical solutions. She can combine a highly technical expertise with accurate planning and leading capabilities. She feels fully satisfied when sees successful results applied to real customers. Irene loves taking responsibility and leading groups to succeed towards a common goal.\nLinkedIn: https://www.linkedin.com/in/irenegonzalvez/\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:597d77d7-f2ff-4373-ad91-16ed1c70031a@597d.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T155500Z
DTEND:20171116T162500Z
SUMMARY:Deriving Actionable Insights from High Volume Media Streams [Technical]
DESCRIPTION:Title:\nDeriving Actionable Insights from High Volume Media Streams\n\nSummary:\nMedia analysts have to deal with with analyzing high volumes of real-time news feeds and social media streams which is often a tedious process because they need to write search profiles for entities. Python tools like NLTK do not scale to large production data sets and cannot be plugged into a distributed scalable frameworks like Apache Flink.\n\nDescription:\nMedia analysts have to deal with with analyzing high volumes of real-time news feeds and social media streams which is often a tedious process because they need to write search profiles for entities. Python tools like NLTK do not scale to large production data sets and cannot be plugged into a distributed scalable frameworks like Apache Flink. Apache Flink being a streaming first engine is ideally suited for ingesting multiple streams of news feeds\, social media\, blogs etc.. and for being able to do streaming analytics on the various feeds. Natural Language Processing tools like Apache OpenNLP can be plugged into Flink streaming pipelines so as to be able to perform common NLP tasks like Named Entity Recognition (NER)\, Chunking\, and text classification. In this talk\, we’ll be building a real-time media analyzer which does Named Entity Recognition (NER) on the individual incoming streams\, calculates the co-occurrences of the named entities and aggregates them across multiple streams\; index the results into a search engine and being able to query the results for actionable insights. We’ll also be showing as to how to handle multilingual documents for calculating co-occurrences. NLP practitioners will come away from this talk with a better understanding of how the various Apache OpenNLP components can help in processing large streams of data feeds and can easily be plugged into a highly scalable and distributed framework like Apache Flink.\n\nSpeaker:\nName: Peter Thygesen\nPosition: Senior Software Engineer @ Paqle A/S \nDescription: Peter is a Big Data enthusiast\, experienced solution architect and developer. He is also a PMC and Committer on Apache OpenNLP\nLinkedIn: https://www.linkedin.com/in/thygesen/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:ac95fbb7-1045-4aca-80b9-55de364b7ae2@ac95.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T155500Z
DTEND:20171116T162500Z
SUMMARY:NRT data enrichment and ingestion with NiFi and Spark [Technical]
DESCRIPTION:Title:\nNRT data enrichment and ingestion with NiFi and Spark\n\nSummary:\n- NiFi\n\nDescription:\nIn order to fulfill increasing needs of business to deliver rich and near real time data for various analysis of Web users behaviour\, the main and complex Apache Spark process was replaced in most parts by more modular and reusable solution built on top of Apache NiFi. Due to this transformation a significant boost and stability of entire process was achieved along with decrease of resources consumption. Also a few approaches were made to ingest the data to Hive in small batches.\n\nSpeaker:\nName: Dawid Węckowski\nPosition: Big Data Engineer @ AGORA SA\nDescription: Dawid works as Big Data Engineer at AGORA SA\, one of the largest media company in Poland. He received his MSc in Computer Science at Poznań University of Economics. His main interests focus on tracking and analysing Web users’ behaviour. Open-source advocate\, fancy exploring new technologies.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 18
UID:306e09cf-06cc-4a70-88d4-8f3cd74d721e@306e.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T155500Z
DTEND:20171116T162500Z
SUMMARY:Spark Streaming + Kafka 0.10: an integration story [Technical]
DESCRIPTION:Title:\nSpark Streaming + Kafka 0.10: an integration story\n\nSummary:\n- Spark\n\nDescription:\nSpark Streaming has supported Kafka since it’s inception\, but a lot has changed since those times\, both in Spark and Kafka sides\, to make this integration more fault-tolerant and reliable.Apache Kafka 0.10 (actually since 0.9) introduced the new Consumer API\, built on top of a new group coordination protocol provided by Kafka itself. So a new Spark Streaming integration comes to the playground\, with a similar design to the 0.8 Direct DStream approach. However\, there are notable differences in usage\, and many exciting new features. In this talk\, we will cover what are the main differences between this new integration and the previous one (for Kafka 0.8)\, and why Direct DStreams have replaced Receivers for good. We will also see how to achieve different semantics (at least one\, at most one\, exactly once) with code examples. Finally\, we will briefly introduce the usage of this integration in Billy Mobile to ingest and process the continuous stream of events from our AdNetwork.\n\nSpeaker:\nName: Joan Viladrosa Riera\nPosition: Senior Big Data Architect and Tech Lead @ Billy Mobile\nDescription: Joan is Senior Big Data Architect and Tech Lead at Billy Mobile where he has been leading the transition to the Hadoop ecosystem during the last two years. Previously\, he worked in Trovit Search\, where he developed Big Data solutions for programmatic buying in SEM platforms like Google AdWords and Microsoft Bing.\nLinkedIn: https://www.linkedin.com/in/joanviladrosa/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:eb1e732c-52be-40a3-b423-96c8c55bed75@eb1e.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T163500Z
DTEND:20171116T170500Z
SUMMARY:Foundations of streaming SQL – learn to love stream & table theory [Technical]
DESCRIPTION:Title:\nFoundations of streaming SQL – learn to love stream & table theory\n\nSummary:\n\n\nDescription:\nWhat does it mean to execute robust streaming queries in SQL? What is the relationship of streaming queries to classic relational queries? Are streams and tables the same thing conceptually\, or different? And how does all of this relate to the programmatic frameworks like we’re all familiar with? This talk will address all of those questions in two parts.\n\nSpeaker:\nName: Tyler Akidau\nPosition: Senior Staff Software Engineer @ Google\nDescription: None\nLinkedIn: https://www.linkedin.com/in/tyler-akidau-5221672/\nTwitter: https://twitter.com/takidau\n        \n        
LOCATION:Theatre 25
UID:242dff42-8505-4afd-a3cc-0e55caab4d95@242d.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T163500Z
DTEND:20171116T170500Z
SUMMARY:Engineering analytics for predictive health applications [Technical]
DESCRIPTION:Title:\nEngineering analytics for predictive health applications\n\nSummary:\n- Agility\n\nDescription:\nAs big data frameworks mature\, solutions build around these technologies provide an increased capability to address challenging engineering problems. There is a need for scaling advanced analytics and for streamlined workflows in tackling big data problems. Using the example of an IoT based application that collects data from a fleet of connected cars and provides insight into their performance and health\, this talk focuses on streamlined workflows for engineering data analytics. It covers a typical lambda architecture that operationalizes machine learning based algorithms. This real-world example also showcases the value of advanced algorithms and simulation in such applications.\n\nSpeaker:\nName: Arvind Hosagrahara\nPosition: Principal Technical Consultant @ MathWorks\nDescription: Arvind Hosagrahara is a principal engineer and a technical lead of a team that specializes in helping organizations use MATLAB algorithms in their business-critical applications. Arvind has extensive hands-on experience developing MATLAB and Simulink applications and integrating them with external technologies. He has helped design the software and workflow for a variety of business-critical applications focusing on robustness\, security\, scalability\, maintainability\, usability\, and forward compatibility across automotive\, energy and production\, finance and other industries.\nLinkedIn: https://www.linkedin.com/in/hosagrahara/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:133567f6-b0ee-4df4-a120-d96e78215aa3@1335.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T163500Z
DTEND:20171116T170500Z
SUMMARY:Make the elephant fly\, once again [Technical]
DESCRIPTION:Title:\nMake the elephant fly\, once again\n\nSummary:\n- Cloud\n\nDescription:\nBol.com has been an early Hadoop user: since 2008 where it was first built for a recommendation algorithm. Since then\, Hadoop has grown to many different teams in the company\, supporting both Business Intelligence but also operation processes or Machine Learning jobs. And now our premise Hadoop cluster has been victim of its success and we face some challenges to run more jobs that use everytime more data without having to invest too much on Hardware and Infrastructure team. That is why we decided in 2017 to move all our Hadoop environment to the Cloud\, not only to be able to easily use more hardware\, but also to use new technologies that outperform our current installation. In this session\, I will present some considerations\, tips\, problems we learned when migrating a heavy used Hadoop cluster to the Cloud\, in order to make the elephant fast again. First I will briefly talked about Bol.com\, the Hadoop technology infrastructure we have and why we want to switch to the Cloud. Then I will focus on Data Quality\, a problem that is quite often overlooked when dealing with lot of data. On that topic\, I will describe an OpenSource project I have built: hive_compared_bq\, a Python program that compares 2 (SQL like) tables\, and graphically shows the rows/columns that are different if any are found. I faced that problem several years ago when working at Hortonworks where out team developed several programs to try to tackle it and thus to ensure that a migration project is successful. I’ll discuss those first approaches\, the problem they have and the advantages we get with hive_compared_bq: straightforward to use\, no need to move the data\, scalable solution that works on the full datasets\, easy way for the developer to see where the erros are. A demo will be made. The second tool I want to present related to Data Quality is DataPrep\, that allows people to quickly see their data\, get an idea about its distribution and spot outliers or errors (also small demo). Another topic I will discuss is Security\, in particularly PII data. Our Hadoop cluster was a no go to store such data because it was too complicated for our infrastructure team to secure it to the standards required by our Security team. I’ll show the advantages we found in the Cloud (encryption\, authorization model\, advanced logs/audit) and the reasons why Security now allows us to store PII data there\, and on which conditions (the BigData components of the Cloud provider have not the same\n\nSpeaker:\nName: Sourygna Luangsay\nPosition: Big Data Architect @ Bol.com\nDescription: Sourygna is a Big Data architect at Bol.com\, the main online webshop in the Netherlands. Previous he worked in Pragsis as a Hadoop infrastructure engineer\, a developer\, a trainer and a manager. And before joining Bol.com he spent some time as a post sales consultant in Hortonworks. Now in Bol.com his focus is switching a bit from Hadoop to some more Cloud native solutions\, but still with Big Data challenges.\nLinkedIn: https://www.linkedin.com/in/sourygna-luangsay-71686b9/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:8d822374-ec79-41b2-a966-d68416a27733@8d82.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T163500Z
DTEND:20171116T170500Z
SUMMARY:The battle of stream processing frameworks [Technical]
DESCRIPTION:Title:\nThe battle of stream processing frameworks\n\nSummary:\nData processing is one of the key components of any data driven system. Right information at right time can impact business to such extent that the business can grow or get left out in the dust of the competitors. In this presentation we will fight the battle of numbers between Apache Spark\, Apache Kafka and Apache Flink and crown the king of stream processing by multiple categories. At the end\, you will be able to pick the right one to solve the problem at hand.\n\nDescription:\nStream processing is a hot topic at the moment and since there are bunch of technologies providing stream processing (some more some less) it became a hot mess. Some of the current frameworks have stream processing as an added feature\, some are a natural fit and some are simply streaming by design. One of the most heavily used data processing frameworks in the last few years is Apache Spark and Spark streaming was added far down the road as a micro-batching functionality. Apache Kafka was initially built as a distributed\, event based data ingestion framework but slowly became a streaming processing platform because of the high requirements for stream processing capabilities. Apache Flink is basically the new kid on the block but is a true streaming first framework\n\nSpeaker:\nName: Matija Gobec\nPosition: Data Engineer @ SmartCat.io\nDescription: Data engineer @ SmartCat\, Apache Cassandra MVP\, distributed systems and automation\nLinkedIn: https://www.linkedin.com/in/matijagobec\nTwitter: https://twitter.com/mad_max0204\n        \n        
LOCATION:Theatre 20
UID:4e84a7ca-af99-4d6d-b908-8aa3060b28d3@4e84.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T172000Z
DTEND:20171116T175000Z
SUMMARY:End-to-End “Exactly Once” with Heron & Pulsar [Technical]
DESCRIPTION:Title:\nEnd-to-End “Exactly Once” with Heron & Pulsar\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Ivan kelly\nPosition: Software Engineer @ Streamlio\nDescription: Ivan Kelly is a Software Engineer for Streamlio\, a startup dedicated to providing a next-generation integrated real-time stream processing solution\, based on Heron\, Apache Pulsar (incubating) and Apache BookKeeper. Ivan has been active in Apache BookKeeper since its very early days as a project in Yahoo! Research Barcelona. Specializing in replicated logging and transaction processing\, he is currently focused on Streamlio’s storage layer.\nLinkedIn: https://www.linkedin.com/in/ivanbkelly/\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:48c6b24f-fe3e-48e0-bd51-0af60bbb6ac6@48c6.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T172000Z
DTEND:20171116T175000Z
SUMMARY:Shipping ML to production: From Data Scientist to End User [Technical]
DESCRIPTION:Title:\nShipping ML to production: From Data Scientist to End User\n\nSummary:\n- Machine Learning\n\nDescription:\nIt has been a long time since the Turing test laid down the first foundations of the philosophy of Artificial Intelligence (AI). AI however has only recently reached the mainstream market\, with applications in the industrial and consumer markets. This talk will take a look at a few specific techniques and architectures that help reduce the time and complexity of iterations when using Machine Learning algorithms to deliver value to end users using Big Data. As the cost of storing data today is rather cheap\, companies usually store a huge volume. Consequently\, the cost of training Machine Learning models to serve clients and the public in general is high because it takes time and uses up a lot of resources due to the high amount of data. To avoid this\, companies need a distributed environment and speedup techniques to reduce the time and cost of training – for example\, a distributed processing framework such as Spark. But once the model has been validated\, the requirements to bring it to production are different. The resources required to provide predictions to the user do not depend on the model itself\, as it has already been trained\, but on the business part of the company and the number of users that need to receive information. A more appropriate architecture for this purpose may be more similar to a microservice than to a distributed processing framework. Thus different solutions should be used for both stages: while Apache Spark should be used for the former\, a microservice should be used for the latter. This would allow companies to easily and rapidly iterate over models to keep them updated. This is indeed another challenge: As new data enters continuously\, how can the model in production be updated at the same time\, including both the historic and new data gathered? Model training uses a lot of resources and is a time-consuming operation due to the large amount of data to be processed. This training process cannot be launched from scratch every time the model has to be updated with new data\, so different solutions are needed – and this goes for all situations in which a same trained model can be used. For example\, training a model to recognize cars in images could also then be used as a basis for recognizing other types of vehicles.\n\nSpeaker:\nName: Mateo Alvarez\nPosition: Data scientist @ StratioBD\nDescription: Mateo Álvarez Calvo is an Aerospace engineer with a Master’s Degree in Propulsion Systems from the Universidad Politécnica de Madrid\, and a Master’s in Data Science from the Universidad Rey Juan Carlos. He is passionate about data analysis\, data science and technology and is currently working as part of the Data Science team at Stratio Big Data\, working on the development of Stratio’s Intelligence module and algorithms applied to Big Data problems and based on Spark.\nLinkedIn: https://www.linkedin.com/in/mateo-alvarez/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:de028ce7-2095-4968-be7f-74960d7009e5@de02.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T172000Z
DTEND:20171116T175000Z
SUMMARY:Scaling a backend for a big data and blockchain environment [Technical]
DESCRIPTION:Title:\nScaling a backend for a big data and blockchain environment\n\nSummary:\n- Blockchain\n\nDescription:\n2gether is a financial platform based on Blockchain\, Big Data and Artificial Intelligence that allows interaction between users and third-party services in a single interface. With that\, users can create his own custom bank. In addition\, the use of blockchain technology makes possible to contract and exchange non-financial assets: electricity\, gasoline…. Thanks to the use of Machine Learning and Big Data technologies\, 2gether offers services that makes the banking experience better and easier. But 2gether is a mobile platform with a backend\, the integration and scalability of all its services and technologies is a real challenge.\n\nSpeaker:\nName: Rafael Ríos Moya\nPosition: Software Architect @ BI Geek & 2getherbank.\nDescription: Software architect passionate about technologies\, after the last 6 years of professional experience he has had the opportunity to participate in several projects that involve the creation of a product right from the start. Thanks of this he recolects experience in ambits such as\, microservices architectures\, service oriented architectures\, distributed architectures\, and infrastructure management.\nLinkedIn: https://www.linkedin.com/in/rafael-r%C3%ADos-moya-16053742/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:d5675ef3-5f75-4411-8d3b-c994fe4973fb@d567.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T172000Z
DTEND:20171116T175000Z
SUMMARY:Streaming analytics better than batch – when and why [Technical]
DESCRIPTION:Title:\nStreaming analytics better than batch – when and why\n\nSummary:\n- Batch\n\nDescription:\nWhile a lot of problems can be solved in batch\, the stream processing approach currently gives you more benefits. And it’s not only sub-second latency at scale. But mainly possibility to express accurate analytics with little effort – something that is hard or usually ignored with older batch technologies like Pig\, Scalding\, Spark or even established stream processors like Storm or Spark Streaming. In this talk we’ll use a real-world example of user session analytics (inspired by Spotify). We’ll give you a use-case driven overview of business and technical problems that modern stream processing technologies like Flink help you solve\, and benefits you can get by using them today for processing your data as a stream.\n\nSpeaker:\nName: Dawid Wysakowicz\nPosition: Data Engineer @ GetInData\nDescription: Dawid Wysakowicz works as a Data Engineer who help people and companies succeed with Apache Flink. He is also active member of the Flink community and he has already contributed numerous patches to Flink (mostly to Flink CEP). First interested with Big Data technologies in 2015 while writing Master Thesis on Distributed Genomic Datawarehouse.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 20
UID:56b3fca2-1552-4851-a54a-b8d2ab3f869b@56b3.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T180000Z
DTEND:20171116T183000Z
SUMMARY:Deep Learning in Spark with BigDL [Technical]
DESCRIPTION:Title:\nDeep Learning in Spark with BigDL\n\nSummary:\n- Spark\n\nDescription:\nBigDL is a deep learning framework modeled after Torch and open-sourced by Intel in 2016. BigDL runs on Apache Spark\, a fast\, general\, distributed computing platform that is widely used for Big Data processing and machine learning tasks. Apache Spark comes with its own machine learning library of algorithms\, but it still lacks deep learning capabilities. BigDL efficiently fills this void by providing rich deep learning support and high performance through Intel’s Math Kernel Library and multi-threaded task execution. In this talk I will give a short overview of Apache Spark\, a description of BigDL architecture and API\, and then I will show a couple of demo examples of using BigDL on some real data.\n\nSpeaker:\nName: Petar Zecevic\nPosition: CTO @ SV Group\nDescription: Petar Zecevic has been working in the software industry for more than 15 years\, as a full-stack developer\, consultant\, analyst\, and team leader. Petar is the author of Spark in Action book (Manning\, September 2016). He also gives talks on Apache Spark\, organizes monthly Apache Spark Zagreb meetups\, and has several Apache Spark projects behind him.\nLinkedIn: https://hr.linkedin.com/in/pzecevic\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:47c263c4-a9e0-41dc-aa5a-ad0178e65ec3@47c2.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T180000Z
DTEND:20171116T183000Z
SUMMARY:Data science for lazy people\, Automated Machine Learning [Technical]
DESCRIPTION:Title:\nData science for lazy people\, Automated Machine Learning\n\nSummary:\nThe power of this new set of tools for Data Science. Is really easy to start applying these technics in your current workflow.\n\nDescription:\nData science is fun… right? Data cleaning\, feature selection\, feature preprocessing\, feature construction\, model selection\, parameter optimization\, model validation… oh wait… are you sure? What about automating 80% of the work even doing better choices than you? Automated Machine Learning has arrived to be your personal assistant in Data Science\n\nSpeaker:\nName: Diego Hueltes\nPosition: Python Engineer @ RavenPack\nDescription: Diego is a computer engineer at Ravenpack (Marbella\, Málaga\, Spain) \, he collaborates teaching in the Big Data Executive Program at Escuela de Organización Industrial (EOI) and also as a Big Data mentor. Diego is passionate about Data Analysis and Big Data\, he loves to share his passion speaking in congress like PyCon Spain 17\, PyData Barcelona 17\, PyCon Spain 16\, the VII Statal RITSI Congress Madrid or in seminars for Granada University Masters.\nLinkedIn: https://www.linkedin.com/in/j-diego-hueltes-vega-485ab553/?ppe=1\nTwitter: https://twitter.com/JdiegoH\n        \n        
LOCATION:Theatre 19
UID:3a02651e-803d-4df8-9738-e0e96016763a@3a02.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171116T180000Z
DTEND:20171116T183000Z
SUMMARY:Video recommendations and Machine Learning [Business]
DESCRIPTION:Title:\nVideo recommendations and Machine Learning\n\nSummary:\nA good content recommendation system is key for any video content provider. Machine Learning video recommendations provide a unique opportunity for broadcasters\, Pay-TV operators\, TV Networks\, and any content distributor to increase engagement and reduce churn through content personalization.\n\nDescription:\nA good content recommendation system is key for any content provider. Machine Learning video recommendations provide a unique opportunity for broadcasters\, Pay-TV operators\, TV Networks\, and any content distributor to increase engagement and reduce churn through content personalization. Currently\, most recommendation systems operate using explicit information provided by the user about their preferences (for example\, by scoring previously watched content) using a technique known as collaborative filtering. Most recommendation systems may also factor in a user’s profile when suggesting appropriate content\, and in doing so may use the preference and feedback information provided by similar users. Such user profile information may contain demographic and geographic data\, in addition to more dynamic data\, such as the user’s web activity (e.g. pages visited\, videos watched\, activity on social networks). Recommendation engines also use techniques that are based on the similarity of pieces of content\, data that is used to make recommendations based on previously watched content. Video Recommendation Technology Today’s video recommendation technology uses machine learning to train\, predict and provide video recommendations to video service users. For the most part\, it uses algorithms to identify item similarity complemented with the user’s view history to produce a recommendation. Item similarity: Users who liked this might also like… If a user shows interest in specific content\, similar content can be recommended via a non-personalised but effective recommendation algorithm known as a content-based-type recommendation. The basic algorithm works as follows: – To measure how similar two given items of content are\, a feature vector\, which encodes different scored metadata (E.g. genre information) should be assigned to each content and should compute the angle between each pair of vectors in Euclidean space. The smaller the angle between the vectors\, the more similar the content is. – Given an item of content\, a shortlist of recommended similar items is produced by finding the most popular and best-rated examples among the most similar content.\n\nSpeaker:\nName: Jeronimo Macanas\nPosition: CEO & Co-founder @ JUMP TV SOLUTIONS\nDescription: Digital entrepreneur passionate about media and entertainment. Almost 20 years of global experience in Over-the-top Video services\, growing businesses from the greenfield to profitability. Top network across the global media and entertainment ecosystem with proven track record in Europe and Latin America. Deep knowledge about Internet TV and Video applications from Business Plan\, design\, architecture definition to development on all platforms from mobile\, game console\, smart TV and STB. Extensive experience making OTT business with Tier 1 TV Networks\, Pay TV Operators and Broadcasters globally. Hands-on startup leader\, involved in all operational aspects of growing a business. Specialties: – Growing business from the greenfield to profitability – Video focused Start-ups – Artificial intelligence and Big Data – Business development – Internet TV/Video business models – Strategic partnerships – Connected Devices Lived in Silicon Valley (San Francisco) and Spain.\nLinkedIn: https://www.linkedin.com/in/jmacanas/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:b1853b8f-2e4b-4f5e-ba08-566210eb4ed4@b185.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T100000Z
DTEND:20171117T103000Z
SUMMARY:The impact of Big Data and applied analytics along the value chain [Business]
DESCRIPTION:Title:\nThe impact of Big Data and applied analytics along the value chain\n\nSummary:\n\n\nDescription:\nGuy Peri\, Chief Data Officer at The Procter & Gamble Company\, will share how big data and applied analytics can have a real impact on how you optimize supply chain and connect with customers. Learn about some of the industry’s biggest challenges and find out what skills and experiences you need to be a key player in transforming businesses through technology\, data\, and analytics.\n\nSpeaker:\nName: Guy Peri\nPosition: Vice President of Information Technology @ Procter & Gamble\nDescription: None\nLinkedIn: https://www.linkedin.com/in/guy-peri-aba291/\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:f9923306-d4a0-42f1-84da-90b408daf0c5@f992.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T104000Z
DTEND:20171117T111000Z
SUMMARY:Turn GDPR’s accountability principles into an added-value for your business [Technical]
DESCRIPTION:Title:\nTurn GDPR’s accountability principles into an added-value for your business\n\nSummary:\n\n\nDescription:\nNone\n\nSpeaker:\nName: Andy Petrella\nPosition: CEO & Founder @ Kensu\nDescription: None\nLinkedIn: https://www.linkedin.com/in/andypetrella/\nTwitter: https://twitter.com/noootsab\n        \n        
LOCATION:Theatre 25
UID:360754bc-f78d-4c70-9193-77ef33cfdf96@3607.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T104000Z
DTEND:20171117T111000Z
SUMMARY:Causal Consistency For Large Neo4j Clusters [Technical]
DESCRIPTION:Title:\nCausal Consistency For Large Neo4j Clusters\n\nSummary:\nAn overview of the Raft algorithm and how Neo4j uses it to provide strong consistency at scale.\n\nDescription:\nAn overview of the Raft algorithm and how Neo4j uses it to provide strong consistency at scale.In this talk\, we’ll explore the new Causal clustering architecture for Neo4j. We’ll see how Neo4j uses the Raft protocol for a robust underlay for intensive write operations\, and how the asynchronous new scale-out mechanism provides enormous capacity for very demanding graph workloads.\n\nSpeaker:\nName: Jim Webber\nPosition: Chief Scientist @ Neo4j\nDescription: None\nLinkedIn: \nTwitter: https://twitter.com/jimwebber?lang=en\n        \n        
LOCATION:Theatre 19
UID:9933d0a7-26c4-4f8e-a2c4-e0d9e17a2dd6@9933.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T104000Z
DTEND:20171117T111000Z
SUMMARY:Tips & Tricks for better Spark applications [Technical]
DESCRIPTION:Title:\nTips & Tricks for better Spark applications\n\nSummary:\n- Spark\, tips\n\nDescription:\nSince its apparition in 2014 Spark has become one of the main tools for Big Data development. It is widely used for batch and realtime applications\, ETL jobs and for building and using machine learning models. It plays a big role in all the major Hadoop distributions and has been incorporated in many products and frameworks. I will talk about the lessons we learned from our Spark projects and I will share several Spark tips and tricks and best practices that can help you get through your Spark projects on time. My book Spark in Action contains a thorough overview of Apache Spark.\n\nSpeaker:\nName: Petar Zecevic\nPosition: CTO @ SV Group\nDescription: Petar Zecevic has been working in the software industry for more than 15 years\, as a full-stack developer\, consultant\, analyst\, and team leader. Petar is the author of Spark in Action book (Manning\, September 2016). He also gives talks on Apache Spark\, organizes monthly Apache Spark Zagreb meetups\, and has several Apache Spark projects behind him.\nLinkedIn: https://hr.linkedin.com/in/pzecevic\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:bcafa63a-b43e-48e7-a833-312262d52ac3@bcaf.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T104000Z
DTEND:20171117T111000Z
SUMMARY:Feature selection for Big Data: advances and challenges [Technical]
DESCRIPTION:Title:\nFeature selection for Big Data: advances and challenges\n\nSummary:\n- Feature selection\n\nDescription:\nIn an era of growing data complexity and volume and the advent of Big Data\, feature selection has a key role to play in helping reduce high-dimensionality in machine learning problems. Feature selection has been successfully applied for years as preprocessing step to different scenarios involving huge amounts of data\, from DNA microarray analysis to face recognition. However\, the current Big Data scenario\, in which datasets will only continue to grow in size and number\, offers both opportunities and challenges to researchers\, as there is an increasing need for scalable yet efficient feature selection methods.\n\nSpeaker:\nName: Verónica Bolón-Canedo\nPosition: Assistant Professor @ University of A Coruna\nDescription: Verónica Bolón-Canedo received her B.S. (2009)\, M.S. (2010) and Ph.D. (2014) degrees in Computer Science from the University of A Coruña (Spain). After a postdoctoral fellowship in the University of Manchester\, UK (2015)\, she is currently an Assistant Professor in the Department of Computer Science of the University of A Coruña. She has extensively published in the area of machine learning and feature selection. On these topics\, she has co-authored one book\, three book chapters and more than 50 research papers in international conferences and journals.\nLinkedIn: https://www.linkedin.com/in/veronicabolon/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:711ba83f-f428-486c-9b33-118080960411@711b.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T112000Z
DTEND:20171117T114500Z
SUMMARY:Roundtable – Friday [Business]
DESCRIPTION:Title:\nRoundtable – Friday\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Óscar Mendez\nPosition: CEO and Co-founder @ Stratio\nDescription: Oscar Mendez Soto is CEO and co-founder of Stratio\, a pioneering Big Data product company founded in 2014. He has more than 15 years of experience launching and managing successful companies and has accrued extensive knowledge in the field of Big Data and Artificial Intelligence over the last two decades. Oscar has always envisaged where technology is going next and has been an early adopter of Big Data technologies that provide massive data processing capabilities. He is the founder of Big Data Spain\, the second biggest Big Data conference in Europe.\nLinkedIn: https://www.linkedin.com/in/oscar-m%C3%A9ndez-soto-8a00a7b/\nTwitter: https://twitter.com/omendezsoto\n        \n        
LOCATION:Theatre 25
UID:e3772a29-cdb4-4b8b-9aca-b94f7892f33a@e377.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T120000Z
DTEND:20171117T123000Z
SUMMARY:The Data Errors we Make [Technical]
DESCRIPTION:Title:\nThe Data Errors we Make\n\nSummary:\nWhere statistical errors come from\, how they cause us to make bad decisions\, and what to do about it.\n\nDescription:\nWith the surging use of data for decision making\, we are forced to confront that there is error in almost everything we measure and every prediction we make. While most people learn about Type I/II errors and are comfortable evaluating machine learning models performance\, there are many more ways for errors to propagate in data science systems that cause bad decisions or product experiences. In this talk\, I provide a generic framework for thinking about errors in data science systems. I show that anticipating errors and quantifying our uncertainty can help us use data more effectively.\n\nSpeaker:\nName: Sean J. Taylor\nPosition: Data Scientist @ Facebook\nDescription: Sean J. Taylor is a computational social scientist on Facebook’s Data Science team.  Prior to Facebook\, he earned his PhD in Information Systems from NYU’s Stern School of Business.  He specializes in using machine learning methods and randomized experiments for measurement\, prediction\, and policy decisions.  Sean’s research ranges from studying online social influence\, viral marketing\, and social networks to measuring how sports fans behave and the impact of data science on decision making in organizations.  He is also an avid engineer who enjoys putting academic research into practice by building tools and services like new kinds of prediction markets and automated forecasting systems.\nLinkedIn: https://www.linkedin.com/in/seanjtaylor/\nTwitter: https://twitter.com/seanjtaylor\n        \n        
LOCATION:Theatre 25
UID:e33fdc88-d092-4095-92db-0c5c9497f885@e33f.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T120000Z
DTEND:20171117T123000Z
SUMMARY:Monitoring world geopolitics through Big Data [Business]
DESCRIPTION:Title:\nMonitoring world geopolitics through Big Data\n\nSummary:\nInformation from the media enables us to answer question which couldn't be solved before.  Our developed tools lets us to quantify events in real time supposing a milestone in the geopolitical arena.\n\nDescription:\nThe geopolitical panorama in 2017 and last years is dominating the world agenda and posing tough challenges for the entire world. Given the increasing importance and economic relevance of geopolitical analysis\, the fact that risks have become mostly global with broad and rapid propagation capacity and that we don’t have rich data sets to measure\, track and describe geopolitical activity\, we have developed a set of new tools to track and quantify the key geopolitical trends and their interconnections in the global economy using GDELT (Global Database of Events\, Language and Tone). It is a real-time global open-source database of human society according to the world’s news media\, reaching deep into local events\, reactions and emotions of every place of the world in almost real time. All this information is freely available to research\, analyse\, visualise and even forecast human society according to global news coverage. It also includes a comprehensive and high-resolution catalogue of geo-referenced socio-political events from 1979 to the present. GDELT monitors every accessible print\, broadcast and online news report around the globe every 15 minutes in over 100 languages. Information is processed using a vast pipeline of algorithms to identify hundreds of categories of events (from protests to appeals for peace)\, thousands of emotions (from anxiety to happiness) capturing the most important events across the planet\; identifying millions of narrative themes (from women’s rights to clean water access) and the involved people\, organizations\, locations\, themes\, as well as the tone and emotions of each article. This innovative database allows us to release several tools by means of Big Data which illustrate our geo-strategic analysis in a visual and comprehensive way in aiming to understand the social\, political and geostrategic trends in parallel with the dynamics of the global economy. The development of these tools has supposed a milestone in the geopolitical analysis\, since it lets us to answer new questions and to measure events which we could not quantify before. Our set of tools ranges from the construction of real time indices such as the conflict and social unrest intensity indices to dynamic maps exploiting the temporal and geographic dimension\, allowing to study potential contagion effects. We also exploit the inter-connections between people\, organizations\, locations\, themes and emotions found in each monitored article to construct networks\, analyse spill-overs effects and identify shocks propagation mechanisms to understand their interconnectedness and explore how risks may propagate. Summing up\, Big Data and data science techniques offer huge opportunities for research in the geopolitical arena. Data from the media allows to enrich our analysis and to incorporate these insights into our models to capture nonlinear behaviour and feedback effects of human interaction\, assessing their global impact on the society and enabling us to construct fragility indices and early warning systems.\n\nSpeaker:\nName: Tomasa Rodrigo\nPosition: Senior Economist @ BBVA\nDescription: Tomasa Rodrigo is currently Head of Big Data at BBVA Research in charge of Big Data projects for geopolitical and economic analysis. She has a large experience working with BigQuery\, GDELT and large databases. She has published several articles about tracking geopolitical\, social and economic events with Big Data.\nLinkedIn: https://www.linkedin.com/in/tomasa-rodrigo-8b0b1392/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:fdb683bf-756b-497d-9c8e-43faddee2fd2@fdb6.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T120000Z
DTEND:20171117T123000Z
SUMMARY:A Deep Learning use case for water end use detection [Technical]
DESCRIPTION:Title:\nA Deep Learning use case for water end use detection\n\nSummary:\nA successful use case of Artificial Intelligence in Industry 4.0.\n\nDescription:\nDeep Learning (DL) is a major breakthrough in artificial intelligence with a high potential for predictive applications. It has been recognized as one of ten breakthrough technologies according to MIT Technology Review[1]. DL has gone from being considered an academic field to being applied in engineering thanks to frameworks like TensorFlow or Theano. In this talk\, we will present a system developed for Canal de Isabel II [2] to identify the residential use of water in its different appliances from records of water meters. This system\, based on a DL approach\, has shown a high potential in smart water management.\n\nSpeaker:\nName: Roberto Díaz\nPosition: Leader of the Data Science research @ Treelogic\nDescription: Dr. Roberto Diaz received his degree in Telecommunications Engineering and a PhD focused on Machine Learning from the university Carlos III de Madrid. He has published relevant papers about efficient implementation of Machine Learning algorithms. He has been involved in many research projects. Currently he is leader of the Data Science research line at Treelogic. He has won several awards thanks to the use of Deep Learning techniques and he has reached the 54th position in the world ranking of data scientists.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 18
UID:9187eb64-f62f-4027-9cf5-ee2243fe7001@9187.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T124000Z
DTEND:20171117T131000Z
SUMMARY:Unified Stream Processing at Scale with Apache Samza [Technical]
DESCRIPTION:Title:\nUnified Stream Processing at Scale with Apache Samza\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Jake Maes\nPosition: Apache Samza Comitter @ LinkedIn\nDescription: Jake is an Apache Samza committer and Software Engineer at LinkedIn. He and his team design\, develop\, and operate the streaming infrastructure that powers near-line applications for derived data\, machine learning\, search indexing\, communication control and many more. Prior to LinkedIn\, he was an Engineer specializing in performance and scalability for IBM’s Cognos and Bluemix.\nLinkedIn: https://www.linkedin.com/in/jacobmaes/\nTwitter: https://twitter.com/jakemaes\n        \n        
LOCATION:Theatre 25
UID:26d0c5ec-c91a-4354-a5c3-5ee5f31ae188@26d0.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T124000Z
DTEND:20171117T131000Z
SUMMARY:Artificial Intelligence tools for developers [Technical]
DESCRIPTION:Title:\nArtificial Intelligence tools for developers\n\nSummary:\n\n\nDescription:\n\n\nSpeaker:\nName: Michael Ludden\nPosition: Director of Product of IBM Watson Developers Labs & AR/VR Labs @ IBM Watson\nDescription: Michael Ludden is Director of Product at IBM Watson Developer Labs & AR/VR Labs. As passionate about solving real problems for developers as he is about Artificial Intelligence\, Michael is also fascinated by all things futurist\, and has been involved in ideation around emerging use cases that include Virtual Reality\, conversational interfaces (digital including “chatbot”​ + physical including “IoT”​)\, and generally the emerging field of AI as a service.\nLinkedIn: https://www.linkedin.com/in/mludden/\nTwitter: https://twitter.com/Michael_Ludden\n        \n        
LOCATION:Theatre 19
UID:bd713741-8da9-4e83-bc42-5f286690a3ec@bd71.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T124000Z
DTEND:20171117T131000Z
SUMMARY:Apache MXNet Distributed Training Explained In Depth [Technical]
DESCRIPTION:Title:\nApache MXNet Distributed Training Explained In Depth\n\nSummary:\n- Mxnet\n\nDescription:\nDistributed training is a complex process that does more harm than good if it not setup correctly. In order to do it right and get the most benefit from it\, one needs to fully understand how it works\, how to effectively debug it\, what are the components that are involved in the process and how they communicate with each other. And I am going to cover all this in my talk.\n\nSpeaker:\nName: Viacheslav Kovalevskyi\nPosition: Software Developer Engineer @ Amazon Deep Learning\nDescription: AWS Deep Learning engineer with 7+ experience in the field. Working on MXNet. Teaching Java courses. In the past has worked with the TensorFlow.\nLinkedIn: https://www.linkedin.com/in/b0noi/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:16a1248b-d047-4bfb-b4db-eeee0fa5a78a@16a1.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T124000Z
DTEND:20171117T131000Z
SUMMARY:Bayesian inference and big data: are we there yet? [Business]
DESCRIPTION:Title:\nBayesian inference and big data: are we there yet?\n\nSummary:\nBayesian inference has a reputation of being complex and only applicable to fairly small datasets. Very recent developments may be chaining that\, and we are starting to see some successful real world application of Bayesian inference to very large datasets\, and some tools to make it accessible to many more data science practitioners.\n\nDescription:\nBayesian inference offers many great features and arguably allow us to extract as much information as possible from real world data\, but it has the reputation of being complex and limited to fairly small datasets. In this talk I will try to challenge both assumptions. First\, I will discuss the “traditional” interpretation of Bayesian probabilities (“a measure of how much we know based on the evidence we’ve been presented”) versus the “modern” interpretation by some authors (“a measure of how trained a model is”)\, and the corresponding view of Bayesian models as machine learning tools\, and inference as model training (LDA being an example of an algorithm that many machine learning practitioners don’t even realize it’s just Bayesian inference over a simple generative model) Then\, I will show the two tradiditional approaches to Bayesian inference\, namely Monte Carlo methods and variational methods\, and their (traditional) drawbacks and limitations. I will explain some advances to overcome those limitations: MCMC\, HMC\, stable Monte Carlo methods with subsampling\, usage of automatic differentiation\, NUTS MCMC\, variational inference\, ADVI\, ADVI with batch processing\, etc. A trend seems to appear that Monte Carlo methods might become suited for “wide” data (when each sample consists in a lot of data) and variational methods for “tall” data (when the number of samples is very large).\n\nSpeaker:\nName: Jose Luis Hidalgo\nPosition: V.P. of Analytics and Data Science @ Nextail Labs\nDescription: Jose Luis holds an M. Eng. in Aeronautical Engineering from Universidad Politécnica of Madrid and a Bachelor in Political Science from UNED. He worked up the consulting ranks in Accenture for nine years before working for technology giants Ericsson and Huawei. Through these positions\, he has managed multi-year digital transformation programs\, held global responsibility for a SaaS CRM product line and led several analytics and machine learning initiatives. Currently\, he holds the position of VP of Analytics and Data Science in Nextail\, a start-up company that applies AI to some big issues in the retail industry.\nLinkedIn: https://www.linkedin.com/in/jlhidalgo/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:93781ff1-5175-4c6f-a201-61da704d28a3@9378.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T132000Z
DTEND:20171117T135000Z
SUMMARY:Flink-Kudu connector: an open source contribution to develop Kappa architectures [Technical]
DESCRIPTION:Title:\nFlink-Kudu connector: an open source contribution to develop Kappa architectures\n\nSummary:\n- Apache Flink\n\nDescription:\nKappa Architecture is a software architecture pattern that makes use of an immutable\, append only log. All the processing of the event will be performed in the input streams and persisted as real-time views. Apache Flink is very well suited to be the processing engine because it provides support for event-time semantics\, stateful exactly-once processing\, and achieves high throughput and low latency at the same time. Apache Kudu Kudu is a storage system good at both ingesting streaming data and good at analysing it using ad-hoc queries (e.g. interactive SQL based) and full-scan processes (e.g Spark/Flink). So Kudu is a good fit to store the real-time views in a Kappa Architecture. We have developed and open-sourced a connector to integrate Apache Kudu and Apache Flink. It allows reading/writing data from/to Kudu using the DataSet and DataStream Flink’s APIs. The connector has been submitted to the Apache Bahir project and is already available from maven central repository.\n\nSpeaker:\nName: Ruben Casado\nPosition: Big Data Manager @ Accenture Digital\nDescription: Rubén Casado is Big Data Manager in Accenture Digital. He received a PhD in Computer Science in 2013. He has worked as a researcher at the University of Oviedo\, Oxford Brookes University and Nancy LORIA/INRIA team. From 2012 to 2016 he was the Head of the Big Data unit at Treelogic. In addition\, Rubén is the organizer of the Meetup Apache Flink Madrid and the director of Master in Big Data Architecture in Kschool.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 25
UID:c8c79e62-9b55-46ef-9ec7-3dcebcdb8dbd@c8c7.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T132000Z
DTEND:20171117T135000Z
SUMMARY:Training Deep Learning Models on Multiple GPUs in the Cloud [Technical]
DESCRIPTION:Title:\nTraining Deep Learning Models on Multiple GPUs in the Cloud\n\nSummary:\nTraining Deep Learning Models on Multiple GPUs in the Cloud\n\nDescription:\nGPUs on the cloud as Infrastructure as a Service (IaaS) seem a commodity. However to efficiently distribute deep learning tasks on several GPUs is challenging. Even if some frameworks offer ways to benefit from data parallelism\, devil is in details. Experts in deep learning care about learning rate and batch sizes. But some engineering details can ruin the scalability or efficiency of their training. Besides software implementation issues\, latency on communications\, GPUDirect support\, drivers configuration or even the pricing of cloud providers can have big impact on training times and costs. Results on this topic will be shared.\n\nSpeaker:\nName: Enrique Otero\nPosition: Data Scientist @ BEEVA\nDescription: Enrique is Data Scientist @ BEEVA. He holds a MSc in Telecommunications Engineering. Postgraduate studies on Information Technology and Communications. Interested on optimization and parallel architectures. And also in shooting down barriers between Data Science and Data Engineering.\nLinkedIn: https://www.linkedin.com/in/enrique-otero-muras-0aab1a133/\nTwitter: \n        \n        
LOCATION:Theatre 19
UID:d8f252dd-2933-4e80-81c3-fd3340da91f2@d8f2.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T132000Z
DTEND:20171117T135000Z
SUMMARY:Vehicle Big Data that Drives Smart City Advancement [Technical]
DESCRIPTION:Title:\nVehicle Big Data that Drives Smart City Advancement\n\nSummary:\n- Big Data\n\nDescription:\nGeotab is a leader in the expanding world of Internet of Things (IoT) and telematics industry with Big Data. From temperature sensors\, CO2 emissions data and pothole detection to preventative maintenance\, our GO devices track this through an Open Platform. Implementing an Open Platform is crucial to the successful advancement of Big Data in an IoT world. Geotab is able to transform businesses and societies globally by collecting nearly 2 billion data points per day. When compounded with our Open Platform\, these data points provide dynamic-insights and urban analytics creating socio economic benefit on urbanization\, safety\, Smart Cities\, and more. However\, what exactly are Smart Cities? What role does Big Data play in them?\n\nSpeaker:\nName: Mike Branch\nPosition: Vice President Business Intelligence @ Geotab\nDescription: With his experience in software design\, data visualization\, and business strategy\, Mike Branch is instrumental in helping make Geotab the most powerful and user-friendly fleet management software platform in the industry. As Vice President of Business Intelligence\, Mike sets the vision for how the nearly 2 billion telematics records processed by Geotab daily are transformed into useful reporting and analytics tools that help customers better understand their businesses. Prior to Geotab\, Mike was the founder and CEO of Inovex Inc.\, a software development firm which specialized in designing information systems for the healthcare and energy sectors. In 2013\, Mike launched Maps BI\, a cloud-based SaaS combining interactive mapping\, business intelligence\, and collaboration. Maps BI was integrated into Geotab’s telematics platform as a Geotab Marketplace partner and then acquired by the company in 2016.\nLinkedIn: https://www.linkedin.com/in/inovexmike/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:42bbeb89-5ac7-41f4-a595-4d13158faf00@42bb.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T132000Z
DTEND:20171117T135000Z
SUMMARY:Trading at market speed with the latest Kafka features [Technical]
DESCRIPTION:Title:\nTrading at market speed with the latest Kafka features\n\nSummary:\n- Kafka streams\n\nDescription:\nNot long ago only banks and hedge funds could afford doing automated and High Frequency Trading\, that is\, the ability to send buy commodities in microseconds intervals. High Frequency needed both expensive hardware\, communications\, and algorithms: something small investors couldn’t afford. That’s not true anymore: Big Data has democratized investing. In this talk you’ll learn how to build your own High-Frequency-Trading platform using Kafka\, Kafka Streams exactly-once-processing\, and KSQL. We will target one of the most exciting markets today: bitcoin and cryptocurrency trading.\n\nSpeaker:\nName: Iñigo González\nPosition: Data & Machine Learning Product Owner @ Mobile One2One\nDescription: Data & Machine learning product owner at Mobile One2One. Data Science Awards 2016: Best Data Engineer prize winner. Also\, the guy who hated databases most in college.\nLinkedIn: https://www.linkedin.com/in/igponce/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:cd06c9ed-ae0f-4af1-a191-b6eb35064935@cd06.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T140000Z
DTEND:20171117T143000Z
SUMMARY:Full Stack Analytics on Amazon Web Services [Technical]
DESCRIPTION:Title:\nFull Stack Analytics on Amazon Web Services\n\nSummary:\n\n\nDescription:\nBuilding analytics applications requires more than just one good service. It requires the ability to capture a vast amount of data\, and react to data changes in real time. It requires flexible tools that enable end users to work in the way they can be most productive\, and that address the needs of both data consumers and data scientists. This analysis won’t just be about data exploration and reports\, but must be able to support the largest\, most complex machine and deep learning models imaginable. Across it all\, strong governance\, security\, and cataloguing is essential.\n\nSpeaker:\nName: Ian Robinson\nPosition: Principal Solution Architect\, Big Data and Analytics. @ Amazon Web Services\nDescription: \nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 25
UID:78c41ad5-1e42-4389-8979-332005c07e00@78c4.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T140000Z
DTEND:20171117T143000Z
SUMMARY:The Analytic Platform behind IBM’s Watson Data Platform [Technical]
DESCRIPTION:Title:\nThe Analytic Platform behind IBM’s Watson Data Platform\n\nSummary:\n- Data Science\,\n\nDescription:\nIBM has built a “Data Science Experience” cloud service that exposes Notebook services at web scale. Behind this service\, there are various components that power this platform\, including Jupyter Notebooks\, an enterprise gateway that manages the execution of the Jupyter Kernels and an Apache Spark cluster that power the computation. In this session we will describe our experience and best practices putting together this analytical platform as a service based on Jupyter Notebooks and Apache Spark\, in particular how we built the Enterprise Gateway that enables all the Notebooks to share the Spark cluster computational resources.\n\nSpeaker:\nName: Luciano Resende\nPosition: Architect @ IBM\nDescription: Luciano Resende is an Architect at IBM Spark Technology Center. He has been contributing to open source at The ASF for over 10 years\, he is a member of ASF and is currently contributing to various big data related Apache projects including Apache Spark\, Apache Zeppelin\, Apache Bahir\, Apache Toree and Apache SystemML. Luciano is the project chair for Apache Bahir\, and also spend time mentoring newly created Apache Incubator projects. Recently\, Luciano has started contributing to Jupyter Ecosystem projects around Enterprise Notebook Platform. At IBM\, he contributed to several IBM big data offerings\, including BigInsights\, IOP and its respective Bluemix Cloud services.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 19
UID:b8fddf4c-b71f-4f57-8a63-daeabb5f2cf4@b8fd.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T140000Z
DTEND:20171117T143000Z
SUMMARY:The future of Hadoop security and its evolution [Technical]
DESCRIPTION:Title:\nThe future of Hadoop security and its evolution\n\nSummary:\n- Hadoop\n\nDescription:\nThis talk defines the state of the art for Hadoop security and describes the planned security features to be added. Hadoop initially was not designed with security in mind\, multiple security features had being developed for some components without designing and integrated security architecture. Identity management\, authorization\, authentication\, encryption at rest\, encryption in the wire and security audit are some covered dimensions. Today the security resides in components such as Sentry for centralized authorization\, Kerberos for authentication\, HDFS encryption for at rest encryption and TLS for in transit encryption. Some companies offer security features to complement the security such as data masking and key management and all these efforts are yet not enough to satisfy the security requirements that Big Data platforms have for Compliances and Cloud security.\n\nSpeaker:\nName: Alejandro Gonzalez\nPosition: Senior Software Engineer @ Cloudera\nDescription: Cloudera’s senior security software engineer. Winner of the National Award of Science and Technology in Mexico\nLinkedIn: https://www.linkedin.com/in/kozlex/\nTwitter: \n        \n        
LOCATION:Theatre 18
UID:ec309577-0ac1-472f-94d0-357585e171ad@ec30.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T140000Z
DTEND:20171117T143000Z
SUMMARY:State of the art time-series analysis with deep learning [Technical]
DESCRIPTION:Title:\nState of the art time-series analysis with deep learning\n\nSummary:\n- Machine learning\n\nDescription:\nTime series related problems have traditionally been solved using engineered features obtained by heuristic processes. Based on the recent success of recurrent neural networks for time series domains\, we present a generic deep learning framework for time series based on convolutional and LSTM recurrent units\, which: (i) works on raw temporal data\; (ii) does not require expert knowledge in designing features\; and (iii) explicitly models the temporal dynamics of features. We detail the architecture and present an actual implementation of such framework. We illustrate the performance of the framework on a use case: the problem of human activity recognition.\n\nSpeaker:\nName: Javier Ordóñez\nPosition: Lead Data Scientist @ StyleSage\nDescription: Javier is the current Lead Data Scientist at StyleSage. He is a cross-disciplinary data scientist with 9 years of experience in the areas of technology\, research\, teaching and consultancy. He is currently focused on designing\, implementing\, and deploying different machine learning systems aimed to categorize products and extract fashion insights. He has been involved in different technology areas\, ranging from the connected car to wearable devices\, across Europe (Spain\, UK\, and The Netherlands). His work includes collaborations with top tech companies\, such as Qualcomm\, Google\, and Huawei. His research interests focus mostly on ubiquitous computing\, wearable sensing and human-behaviour modelling\, with relevant papers published in premier conferences and journals. Javier has a M. Eng.\, M.Sc and PhD\, with honors\, in Computer Science from Carlos III University of Madrid\nLinkedIn: https://www.linkedin.com/in/fjordonez/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:e71fc259-536f-46d1-a7c4-91d2be020af6@e71f.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T144000Z
DTEND:20171117T151000Z
SUMMARY:Boost Hadoop and Spark with in-memory technologies [Technical]
DESCRIPTION:Title:\nBoost Hadoop and Spark with in-memory technologies\n\nSummary:\nIn this presentation\, attendees will see how to speed up existing Hadoop and Spark deployments by just making Apache Ignite responsible for RAM utilization. No code modifications\, no new architecture from scratch!\n\nDescription:\nHadoop and Spark are fast\, but in-memory technologies are blazingly faster. Today\, if there is a requirement to speed-up data access it means we need to do something around HDFS\, add more nodes to the cluster\, purchase more expensive hardware or look for solutions from Hadoop’s ecosystem of technologies. But what if we had some extra RAM in our data warehouse? Could we make use of it? In this presentation\, attendees will see how to speed up existing Hadoop and Spark deployments by just making Apache Ignite responsible for RAM utilization. No code modifications\, no new architecture from scratch! Specifically\, this presentation will cover the following: * Hadoop Accelerator * HDFS compliant In-Memory File System * MapReduce Accelerator * Spark Shared RDDs * Spark SQL boost\n\nSpeaker:\nName: Akmal Chaudhri\nPosition: Technology Evangelist @ GridGain\nDescription: Akmal Chaudhri is a Technology Evangelist at GridGain. His role is to help build the global Apache Ignite community and raise awareness through presentations and technical writing. He has over 25 years experience in IT and has previously held roles as a developer\, consultant\, product strategist and technical trainer. He has worked for several blue-chip companies such as Reuters and IBM\, and also the Big Data startups Hortonworks (Hadoop) and DataStax (Cassandra NoSQL Database). He has regularly presented at many international conferences and served on the program committees for a number of major conferences and workshops. He has published and presented widely and edited or co-edited 10 books. He holds a BSc (1st Class Hons.) in Computing and Information Systems\, MSc in Business Systems Analysis and Design and a PhD in Computer Science. He is a Member of the British Computer Society (MBCS) and a Chartered IT Professional (CITP).\nLinkedIn: https://www.linkedin.com/in/akmalchaudhri/\nTwitter: \n        \n        
LOCATION:Theatre 25
UID:09a30a1b-13c8-4c3b-a689-b043dbf62aad@09a3.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T144000Z
DTEND:20171117T145000Z
SUMMARY:Attacking Machine Learning used in AntiVirus with Reinforcement [Technical]
DESCRIPTION:Title:\nAttacking Machine Learning used in AntiVirus with Reinforcement\n\nSummary:\nIn recent years Machine Learning (ML) and especially Deep Learning (DL) have achieved great success in many areas such as visual recognition\, NLP or even aiding in medical research. The computer security sector has wanted to use the capabilities of these algorithms to develop classifiers to detect malware.  Unfortunately\, ML and DL systems\, often demonstrate incorrect behaviors for several reasons such as overfitting\,  biased training data\, or because they are too linear.  The objetive of this short talk is to show how an attacker could evade an ML-based malware detection system using Reinforcement Learning.\n\nDescription:\nIn Reinforcement Learning there is no clearly defined correct answer to an entry as in Supervised Learning. You will have an agent who has to decide what action to take from the state in which he is to maximize the reward he will receive. To do this\, the agent learns from experience by collecting training examples that will tell you by trial-error what actions were good and what were bad. A specific type of Reinforcement Learning is known as Q-Learning which consists in evaluating what action to take based on an “action-value” function that determines the value of performing a concrete action from a given state. More specifically\, there is a “Q function” that receives as input a state and an action and returns the expected reward of that action in that state. Before scanning the environment\, “Q” returns the same random fixed value. But as this environment is explored more and more\, “Q” provides a better approximation of the value of an action “a” from the state “s”. The Q function will be updated as the training progresses. An improvement in this type of learning that is receiving very good results is achieved through Deep Q-Networks (DQN)\, which employ Deep Learning techniques to approximate non-linear “Q-functions”. To create undetectable malware using Reinforcement Learning\, the environment will be a sample of malware and the agent will be the algorithm whose function is to change the environment. The action policy and the “Q function” will determine what action to take. The set of actions available will be possible modifications that can be made to malware without corrupting it or altering its functionality. The reward function will use the output of the antivirus malware classifier to generate its result. So it will return 0 if the modified malware sample was not detected by that classifier or 1 in case the antivirus detected it as dangerous. This study (based on the paper called “Evading next-gen AV using A.I.”) is focused on attacking antivirus engines that use malware classifiers for Windows PE files during the static analysis of those files. The Portable Executable (PE) format is a file format for executables and DLLs used in 32-bit\n\nSpeaker:\nName: Ruben Martinez\nPosition: Data Scientist @ Datahack\nDescription: Computer Engineer by UPM and Master in Data Science. I have developed courses such as the title of Project Development with UML and Java also taught by UPM\, CEH\, Cloudera Developer Training for Apache Hadoop\, Cloudera Developer Training for Apache Spark\, Introduction to Big Data with Apache Spark (Databricks) among others. I have worked as a security auditor in a startup called StackOverflow as well as professor of the Postgraduate in Computer Security and Systems Hacking in the Polytechnic University School of Mataró or the online superior title of Computer Security and Hacking Systems Ethics of the Rey Juan Carlos University. I have also participated as a co-author of hacking books of Ra-Ma publishing house such as Hacking and Web Page Security (MundoHacker)\, Hacking and Internet Security Ed. 2011\, etc. Also I have developed the University Superior Course in Robot Programming with ROS (URJC). I am currently working as Data Scientist in Datahack.\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 19
UID:7d638fa4-22be-4ac7-b568-1fc208b291dd@7d63.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T144000Z
DTEND:20171117T145000Z
SUMMARY:A matrix approach to the redundancy of vectorial databases [Technical]
DESCRIPTION:Title:\nA matrix approach to the redundancy of vectorial databases\n\nSummary:\nWe propose a mathematical model of the redundancy of a vectorial database in order to obtain an automatized tool that allows us to remove unnecessary data\, computing the level of the redundancy\, and recover the obtained databases. Since this type of databases can be encoded by an oriented directed graph\, we address the problem of measuring and cleaning redundancies by using matrix theory. Algorithms are presented in Python and MapReduce being the first one the most efficient from the computational point of view.\n\nDescription:\nCurrent systems of knowledge extraction are based on the creation of statistical models that solve a specific problem with given data. In addition\, the used algorithms are implemented and applied in different data management and processing architectures\, from the most rudimentary to the most advanced analytical platforms using Big Data at real time. In the context of cyber-security\, the main goal in a smart system is to create models that generate knowledge from cyber-databases of security reports. A cyber-database contains a lot of unstructured information together with a high level of correlations that they are performed by expert human knowledge. In general\, a cyber-database is composed of security reports\, that is\, information that is encoded by vectors with features whose information is about of a security incident. The structural variety of the data of security reports is not unique (from machine generated data to synthetic or artificial data). Moreover\, the value of each feature could be structured\, unstructured or semi-structured\, and these typologies provide quantitative\, (pseudo) qualitative or string features. Regarding the valence of a cyber-database\, this is a dense set of data because we usually find a high ratio of connection. However\, the relations are hidden because each vendor uses a different lexical level to explain incidents in its local log\, and by the own nature of the context. Moreover\, the security and privacy are the most relevant aspects when we want to create smart critical infrastructures that include tools for analysis of security reports. In this case\, we can not use online software because sharing the data is not allowed. If we want to get knowledge from data\, our best chance to get success is to optimize the different phases of the treatment and analysis of the data. In a cybersecurity context\, we usually can not design the data acquisition process. Then\, the task of cleaning data is the first available stage of the procedure in which we can try to improve the efficiency. We focus on computing the level of superficial redundancy of a cyber-database. This type of redundancy includes all variables that we do not need to take into account in our further analysis (empty variables\, constant\, duplicated values\, etc). This study of superficial redundancy allows us to filter the database without advanced statistical analysis. Moreover\, this process could be applied to any type of features without needing any previous encoding or treatment. We not only can compute the level of the redundancy\, we can also obtain the original or filtered cyber-database\, the removed variables and the associated representation of the database\, at any time of the process. Algorithms in Python and MapReduce are given for the removing redundancies problem.\n\nSpeaker:\nName: D. Ángel Luis Muñoz\nPosition: Researcher @ Research Institute of Applied Sciences in Cybersecurity\nDescription: Ángel Luis Muñoz Castañeda got his Ph.D. at Freie Universität Berlin in 2017. His research interests are focused in algebraic geometry and its interactions with engineering\, in particular with coding theory\, linear control systems\, and information geometry. He is currently working as a researcher at the Research Institute of Applied Sciences in Cybersecurity (University of León).\nLinkedIn: \nTwitter: \n        \n        
LOCATION:Theatre 18
UID:360fa979-36c5-4514-82ab-9827a78cd786@360f.org
END:VEVENT
BEGIN:VEVENT
DTSTAMP:20171114T213621Z
DTSTART:20171117T144000Z
DTEND:20171117T145000Z
SUMMARY:Improvement of team management using chatbots in a natural l [Technical]
DESCRIPTION:Title:\nImprovement of team management using chatbots in a natural l\n\nSummary:\nChatbots have become extremely popular thanks to new machine learning technology which allows companies to develop smarter and more useful chatbots. Many companies including Apple\, Microsoft\, Amazon have developed their own chatbots to make our life easier.  BI Geek proposes to use chatbots in a business scope. A chatbot will allow a team to do tasks faster while saving time. The use of the natural language service by IBM Watson and APIs\, which many companies provide\, will allow us to build an interactive chatbot that can streamline our daily tasks.\n\nDescription:\nIn our organization\, we leverage Slack as the primary communication tool between team members. Slack is very convenient for sharing files\, having calls and group discussions. Slack even provides a chatbot tool to develop a customized chatbot. This tool doesn’t support natural language so the conversation with the chatbot is not fluent and other services are necessary to improve its capabilities. Watson is an IBM service capable of answering questions posed in natural languages. Watson provides an API that allows us to use its services in an effortless way. Watson has multiple services such as: conversation\, translation and natural language understanding. In this paper\, we will talk about the conversation service which is used to answer questions in a natural language. In this proposal\, we want to show how important a chatbot tool can be to a team. We are used to using many different applications in a normal day. We usually use the email service to receive and send mails between our team and customers\, the calendar to schedule meetings\, and other applications for many various tasks such as requesting vacations\, tracking project times etc. Utilizing Slack’s ability to customize chatbots\, we can develop a chatbot with the goal of centralizing the different tasks in a single system. Using Watson Conversation service allows us to use natural language conversations improving the efficiency of the conversations. Watson is the centerpiece of our system\, it is able to understand customer requests and classify them among the possible actions to be performed. Watson’s conversation is trained using some example conversations for specific actions. For example\, if someone wants to look at another’s calendar he can write: “How is Smith’s calendar today?”\, “Tell me about Smith’s calendar”\, etc. Using some examples\, Watson will be able to understand other kinds of questions thanks to its machine learning system. Furthermore\, Watson is able to understand some words like time and location. In the previous example\, Watson can understand that the request is for “today” in the calendar and not another date. It is also possible to define words and synonyms. Watson is able to understand some cities\, and streets as locations\, but to schedule a meeting we can define words such as: office\, lunch counter\, 2nd floor\, etc to determine what action is displayed. Two potential actions are Calendar Management and Time Tracking. Calendar Management: Google Calendar API allows us to interact with our team’s calendar. The following actions were defined to implement in the chatbot: -Looking at our personal and other members’ calendars -Find free time for a meeting between member calendars -Schedule a meeting given: Start and end time\, location\, titles and members to send the invitation. All these actions are defined in Watson\, including location and time extraction. Watson understands user input and gives us back the action to execute with some other information (such as word extractions: location\, times\, etc.). Based on these details we can contact the calendar API and execute the corresponding actions. Time tracking: Harvest is one application used for tracking team times. Harvest provides an API service which allows us to interact with the application. Our team must add every day how long time they spent in each project. Members of the team just need to write to the chatbot in which project they were working and for how long and Watson is able to identify the projects and number of hours. Once the chatbot has these values\, it is possible to connect with the Harvest API and submit the new time entry. Thanks to this development\, our team is able to perform daily tasks faster and keep a record in the conversation chatbot of everything done. The idea is to keep centralizing more services in a chatbot to continue improving the efficiency of the members of the team.\n\nSpeaker:\nName: Javier de la Rosa\nPosition: BI & Big Data Senior Consultan @ BI Geek\nDescription: Javier de la Rosa is a software engineer focused on data analytics using ETL processing and Big Data technologies with experience in some of the most important finance entities. His current role is the development and design of architectures in Big Data projects using the latest technologies. He also works defining and developing machine learning models to predict and cluster dataset information.\nLinkedIn: https://www.linkedin.com/in/javier-de-la-rosa-fernandez-7082baa4/\nTwitter: \n        \n        
LOCATION:Theatre 20
UID:39e7e735-721d-49ef-a03f-b72fee26ae50@39e7.org
END:VEVENT
END:VCALENDAR
